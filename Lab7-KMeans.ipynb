{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. 7 - K-Means and the LLoyd algorithm\n",
    "\n",
    "Sina Hatami (S5447389)\n",
    "\n",
    "In this lab we consider the problem of **unsupervised learning**, through one of the most famous **clustering** algorithms: K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels == 0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_distances(X1, X2):\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='sqeuclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Implementing the LLoyd Algorithm\n",
    "\n",
    "The Lloyd algorithm is the standard algorithm for implementing k-means. It is based on two steps\n",
    " 1. Assigning each point to a cluster\n",
    " 2. Updating the cluster centers\n",
    "repeated iteratively until the cluster centers (and point assignments) have converged.\n",
    "\n",
    "The function skeleton below takes as input the following arguments\n",
    " - `X` the data matrix\n",
    " - `centers` the initial cluster centers. This could be random, or as we will see in the second part, they could be initialized following a smarter strategy.\n",
    " - `maxiter` the maximum number of iterations of the algorithm.\n",
    " \n",
    "The function returns\n",
    " - the final cluster centers\n",
    " - the assignment labels of each point to their cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd(X, centers, maxiter):\n",
    "    # X: n x d\n",
    "    # centers : k x d\n",
    "    n, d = X.shape\n",
    "    k = centers.shape[0]\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        # Compute Squared Euclidean distance (i.e. the squared distance)\n",
    "        # between each cluster centre and each observation\n",
    "        dist = # ... fill here ...\n",
    "        \n",
    "        # Assign data to clusters: \n",
    "        # for each point, find the closest center in terms of euclidean distance\n",
    "        c_asg = # ... fill here ...\n",
    "\n",
    "        # Update cluster center\n",
    "        for c in range(k):\n",
    "            centers[c] = # ... fill here ...\n",
    "        \n",
    "    return c_asg, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. K-Means: Analysis\n",
    "\n",
    "Here you should create a synthetic dataset using the `mixGauss` function **with four or more classes**.\n",
    "\n",
    "You can experiment with different dataset creation strategies:\n",
    " - Create datasets where the classes are very well separated, then k-means should be able to infer the classes easily\n",
    " - Create datasets where the classes have overlap. In this case the k-means algorithm won't be able to distinguish the points of overlap. What do you think will happen?\n",
    " \n",
    "Then, you should run the k-means algorithm with randomly initialized centers:\n",
    " 1. Create the random centers **within the same range as your data**. You can use the `np.random.uniform` function for this.\n",
    " 2. Run the Lloyd algorithm\n",
    " 3. Plot the results.\n",
    "\n",
    "##### Your Tasks\n",
    "You should repeat this procedure multiple times, and comment on the following:\n",
    " 1. Do you obtain the same clusters every time?\n",
    "     If you obtain clusterings which are always the same, you can try to initialize two cluster centers at a very similar point. This will put the algorithm in a difficult situation!\n",
    "     \n",
    " 2. Why do the colors of a specific cluster seem to change at each iteration?\n",
    " 3. Try using the *wrong* number of clusters `k` (wrong with respect to the number of classes you used to generate the data). What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset with at least 4 classes in 2 dimensions, and plot it\n",
    "X, Y = mixGauss(# ... fill here ...\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Y, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try with a random initialization of the centers.\n",
    "k = # ... fill here ...\n",
    "centers0 = np.random.uniform(low=np.amin(X, 0), high=np.amax(X, 0), size=(k, X.shape[1]))\n",
    "\n",
    "# Call the lloyd function\n",
    "Iv, centers = # ... fill here ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, marker='*', alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8)\n",
    "\n",
    "# NOTE: TRY MULTIPLE TIMES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. A Better Initialization Strategy: K-Means++\n",
    "\n",
    "Initializing the cluster centers at random, sometimes makes the algorithm converge to a sub-optimal local minimum.\n",
    "\n",
    "The k-means++ algorithm is an **initialization strategy** to generate the initial centers, which can then be passed to the lloyd algorithm.\n",
    "\n",
    "K-means++ works by selecting the cluster centers as the points within the dataset which have the **maximum distance** between each other. To do this it uses a greedy strategy implemented in the `kmeanspp` function below.\n",
    "\n",
    "Take a look at the K-Means++ function and then use it as initialization for the K-means algorithm.\n",
    "\n",
    "##### Your task:\n",
    "You should take a dataset on which K-Means was struggling, and apply K-Means++ for the center initialization, trying to show that the new initialization strategy improves the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(X, k):\n",
    "    n, d = X.shape\n",
    "\n",
    "    IdxC = np.random.permutation(n)\n",
    "    centers = np.zeros((k, d))\n",
    "\n",
    "    # Select a random point in the dataset as the starting point\n",
    "    centers[0, :] = X[IdxC[0], :]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        D = all_distances(centers[:i, :], X)\n",
    "        Ds = np.min(D, axis=0)  # This is the distance to the closest existing center\n",
    "\n",
    "        # Probability of choosing new points as centers is weighted as the \n",
    "        # squared distance to the closest existing center.\n",
    "        D2 = Ds ** 2\n",
    "        P = np.divide(D2, np.sum(D2))\n",
    "        \n",
    "        # Simply pick the point with the highest probability\n",
    "        newcpos = np.argmax(P)\n",
    "        centers[i,:] = X[newcpos, :]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and use K-Means++ for initialization\n",
    "k = # ... fill here ...\n",
    "centers0 = # ... fill here ...\n",
    "\n",
    "# Call the Lloyd function\n",
    "Iv, centers = # ... fill here ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
